{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5 - Trees and SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "This assignment is worth a total of **15 points (10 + 5 bonus)**. The goal of this assignment is to introduce you to decision trees, ensembles of models and SVMs. \n",
    "\n",
    "We have structured the assignment into three major parts:\n",
    "\n",
    "1. **Part One**: Decision Trees,\n",
    "2. **Part Two**: Support Vector Machines\n",
    "3. **Part Three**: Revision of all models \n",
    "\n",
    "To install any unavailable package, you can do it similarly to the previous assignment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Decision Trees, <span style=\"color:green\">total of 5 points </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "RANDOM_STATE = 40\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to plot boundaries of decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(clf, X, y, axes, cmap):\n",
    "    x1, x2 = np.meshgrid(\n",
    "        np.linspace(axes[0], axes[1], 100), np.linspace(axes[2], axes[3], 100)\n",
    "    )\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "\n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=cmap)\n",
    "    plt.contour(x1, x2, y_pred, cmap=\"Greys\", alpha=0.8)\n",
    "    markers = (\"o\", \"^\")\n",
    "    for idx in (0, 1):\n",
    "        plt.plot(\n",
    "            X[:, 0][y == idx],\n",
    "            X[:, 1][y == idx],\n",
    "            marker=markers[idx],\n",
    "            linestyle=\"none\",\n",
    "        )\n",
    "    plt.axis(axes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity of Trees to the Orientation of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, you are hopefully convinced that decision trees offer many advantages: they are relatively easy to understand and interpret, straightforward to use, versatile, and powerful. However, they do come with some limitations. One key drawback is their preference for orthogonal decision boundaries, meaning all splits are perpendicular to an axis. This makes them sensitive to the orientation of the data.\n",
    "\n",
    "### <span style=\"color:red\">Task 1: Fill out the missing code</span>  <span style=\"color:green\">Total: 2 points </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a synthetic dataset\n",
    "np.random.seed(RANDOM_STATE)\n",
    "X_square = np.random.rand(100, 2) - 0.5  # Random points in the range [-0.5, 0.5]\n",
    "y_square = (X_square[:, 0] > 0).astype(np.int64)  # Labels based on x > 0\n",
    "\n",
    "# Apply a 45-degree rotation\n",
    "angle = np.pi / 4  # 45 degrees\n",
    "rotation_matrix = np.array(\n",
    "    [[np.cos(angle), -np.sin(angle)], [np.sin(angle), np.cos(angle)]]\n",
    ")\n",
    "X_rotated_square = X_square.dot(rotation_matrix)\n",
    "\n",
    "# Train a decision tree on the original dataset\n",
    "tree_clf_square = DecisionTreeClassifier(random_state=RANDOM_STATE, max_depth=3)\n",
    "___ # TODO: Fit the decision tree to the original dataset\n",
    "\n",
    "\n",
    "\n",
    "tree_clf_rotated_square =  # TODO: Create a new DecisionTreeClassifier instance\n",
    "___ # TODO: Fit the new tree to the rotated dataset\n",
    "\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "plt.sca(axes[0])\n",
    "plot_decision_boundary(\n",
    "    tree_clf_square, X_square, y_square, axes=[-0.7, 0.7, -0.7, 0.7], cmap=\"Pastel1\"\n",
    ")\n",
    "plt.title(\"Original Data\")\n",
    "plt.sca(axes[1])\n",
    "plot_decision_boundary(\n",
    "    tree_clf_rotated_square,\n",
    "    X_rotated_square,\n",
    "    y_square,\n",
    "    axes=[-0.7, 0.7, -0.7, 0.7],\n",
    "    cmap=\"Pastel1\",\n",
    ")\n",
    "plt.title(\"Rotated Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Task 2: Do PCA first</span>\n",
    "\n",
    "A potential solution to this issue is to apply a principal component analysis (PCA) transformation. Remember: PCA rotates the data to reduce the correlation between features. This process often (though not always) simplifies things for decision trees.\n",
    "\n",
    "#### <span style=\"color:green\">Total: 1 point </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "np.random.seed(RANDOM_STATE)\n",
    "X_square = np.random.rand(100, 2) - 0.5  # Random points in the range [-0.5, 0.5]\n",
    "y_square = (X_square[:, 0] > 0).astype(np.int64)  # Labels based on x > 0\n",
    "\n",
    "# Apply PCA to the dataset\n",
    "pca = # TODO: Create a PCA instance with 2 components\n",
    "X_pca =  # TODO: Fit PCA to the dataset and transform the data to the principal component space\n",
    "\n",
    "# Train a decision tree on the PCA-transformed dataset\n",
    "tree_clf_pca = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf_pca.fit(X_pca, y_square)\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "plot_decision_boundary(\n",
    "    tree_clf_pca, X_pca, y_square, axes=[-1.5, 1.5, -1.5, 1.5], cmap=\"Pastel1\"\n",
    ")\n",
    "plt.title(\"Decision Tree after PCA transformation\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've fitted the decision tree, let's bring it to life with a visual representation. We will plot the tree that was generated from the rotated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(\n",
    "    tree_clf_rotated_square,\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    ")\n",
    "plt.title(\"Decision Tree on Rotated Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This decision tree shows how the dataset is split based on feature values:\n",
    "\n",
    "1. **Root Node**:\n",
    "   - The root node is the starting point. It splits the data into two groups: 54 samples go left (True), and 46 go right (False).\n",
    "   - The Gini index shows the data is quite mixed at the start.\n",
    "\n",
    "2. **Splits**:\n",
    "   - Each node splits the data further based on a feature and a threshold. For example, the second split on the left uses `x[1] <= -0.235`.\n",
    "\n",
    "3. **Colors**:\n",
    "   - The node colors represent which class is dominant:\n",
    "     - **Orange** for one class.\n",
    "     - **Blue** for the other.\n",
    "   - Darker colors mean the node is more pure (contains mostly one class).\n",
    "\n",
    "4. **Leaf Nodes**:\n",
    "   - The tree ends at leaf nodes, where no further splitting is needed. Most of these nodes have Gini = 0.0, meaning they contain samples from only one class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One big tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "X_quad = np.random.rand(200, 1) - 0.5  # a single random input feature\n",
    "y_quad = X_quad**2 + 0.025 * np.random.randn(200, 1)\n",
    "tree_reg = DecisionTreeRegressor(max_depth=2, random_state=RANDOM_STATE)\n",
    "tree_reg.fit(X_quad, y_quad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Task 3: Which value should you use?</span> \n",
    "\n",
    "We want to train a Regression Tree that overfits the data and then visualize it.\n",
    "\n",
    "<span style=\"color:green\">Total: 2 points </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg1 = DecisionTreeRegressor(\n",
    "    random_state=RANDOM_STATE, max_depth=# TODO: Set max_depth to a value that causes the tree to overfit (e.g., very high depth or no restriction)\n",
    ")  \n",
    "tree_reg2 = DecisionTreeRegressor(\n",
    "    random_state=RANDOM_STATE, max_depth=5, min_samples_leaf=10\n",
    ")\n",
    "tree_reg1.fit(X_quad, y_quad)\n",
    "tree_reg2.fit(X_quad, y_quad)\n",
    "\n",
    "x1 = np.linspace(-0.5, 0.5, 500).reshape(-1, 1)\n",
    "y_pred1 = # TODO: Predict values of x1 using tree_reg1\n",
    "y_pred2 = # TODO: Predict values of x2 using tree_reg1\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plt.plot(X_quad, y_quad, \"b.\")\n",
    "plt.plot(x1, y_pred1, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
    "plt.axis([-0.5, 0.5, -0.05, 0.25])\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$y$\", rotation=0)\n",
    "plt.legend(loc=\"upper center\")\n",
    "plt.title(\"First Tree\")\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plt.plot(X_quad, y_quad, \"b.\")\n",
    "plt.plot(x1, y_pred2, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
    "plt.axis([-0.5, 0.5, -0.05, 0.25])\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.title(\"Second Tree\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try plotting the tree on the left. It will take some time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(\n",
    "    tree_reg1,\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    ")\n",
    "plt.title(\"Decision Tree on Rotated Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This decision tree is quite large due to its significant depth, which makes it less interpretable. While deeper trees can capture more complex patterns, they also result in more splits and nodes, making the structure harder to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Support Vector Machines, <span style=\"color:red\">Task 4: Fill out the code</span> \n",
    "\n",
    "In this assignment, we will explore the behavior of Support Vector Machines (SVM) on a synthetic, nonlinear dataset. The goal is to understand how different SVM kernels and hyperparameters (specifically, the regularization parameter C) affect the model's performance, especially in the context of nonlinear decision boundaries.\n",
    "\n",
    "<span style=\"color:green\">Total of 2 points </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Create synthetic nonlinear dataset\n",
    "X, y = make_circles(n_samples=100, noise=0.1, factor=0.3, random_state=42)\n",
    "\n",
    "# Step 2: Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Step 3: Fit SVM with different kernels and parameters\n",
    "kernels = [\"linear\", \"poly\", \"rbf\"]\n",
    "C_values = [0.1, 1, 10]\n",
    "fig, axes = plt.subplots(len(kernels), len(C_values), figsize=(18, 12))\n",
    "\n",
    "# Plotting for each kernel and C value\n",
    "for i, kernel in enumerate(kernels):\n",
    "    for j, C in enumerate(C_values):\n",
    "        svm = SVC(kernel=kernel, C=C)\n",
    "        svm.fit(X_train, y_train)\n",
    "        y_pred = # TODO: Use the fitted model to make predictions on the test set\n",
    "        accuracy = # TODO: Compute the accuracy of the model using the test labels and predictions\n",
    "\n",
    "        ax = axes[i, j]\n",
    "        ax.set_title(f\"Kernel: {kernel}, C={C}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        # Scatter plot of test points\n",
    "        ax.scatter(\n",
    "            X_test[:, 0], X_test[:, 1], c=y_test, cmap=\"coolwarm\", edgecolors=\"k\", s=50\n",
    "        )\n",
    "\n",
    "        # Plot decision boundary\n",
    "        h = 0.02\n",
    "        x_min, x_max = X_test[:, 0].min() - 1, X_test[:, 0].max() + 1\n",
    "        y_min, y_max = X_test[:, 1].min() - 1, X_test[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "        Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "\n",
    "        # Contour plot of decision boundary\n",
    "        ax.contourf(xx, yy, Z, alpha=0.8)\n",
    "        ax.set_xlim(X_test[:, 0].min() - 1, X_test[:, 0].max() + 1)\n",
    "        ax.set_ylim(X_test[:, 1].min() - 1, X_test[:, 1].max() + 1)\n",
    "\n",
    "# Adjust layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Real world scenario - Model Comparison Using Multiple Classifiers, <span style=\"color:green\">total of 8 points </span> \n",
    "\n",
    "In this scenario, we are tasked with evaluating multiple classification models on several datasets of varying complexities. These datasets include well-known ones like Iris and Wine, as well as more challenging ones like the Moon and a hard synthetic dataset.\n",
    "\n",
    "### Process Overview\n",
    "Typically, when working on a classification problem, I would follow a structured approach to model selection and tuning:\n",
    "\n",
    "1. **Dataset Preprocesing**: Analyze the characteristics of each dataset, such as the number of features, the distribution of target classes, and the complexity of the relationships between the features. Data preprocessing usually takes the most \"human\" time.\n",
    "2. **Model Selection**: Try different types of models, such as K-Nearest Neighbors (KNN), Logistic Regression, Decision Trees, Random Forests, and Support Vector Machines (SVM). Each model has its strengths and weaknesses depending on the nature of the data. Usually we try the default parameters of each algorithm, and then select a subset of them to hyperparameter tune.\n",
    "3. **Hyperparameter Tuning**: Use techniques like Grid Search to tune the hyperparameters of each model. This process involves selecting the best combination of parameters (such as the number of neighbors in KNN, the regularization strength in Logistic Regression, or the depth of the trees in Decision Trees).\n",
    "4. **Model Evaluation**: Evaluate each model using testing metrics like accuracy, precision, recall, and F1 score. Choose the model that performs best on the test data.\n",
    "\n",
    "\n",
    "### Approach in our case\n",
    "\n",
    "1. **Try Different Models**: We evaluate several models, including KNN, Logistic Regression, Decision Trees, Random Forests, and SVMs, to understand how they perform on each dataset.\n",
    "2. **Hyperparameter Tuning**: We use Grid Search to find the best hyperparameters for each model, adjusting values such as the number of neighbors for KNN, regularization strength for Logistic Regression, and tree depth for Decision Trees.\n",
    "3. **Evaluation**: We measure model performance using accuracy, as well as training time, to ensure efficient results.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "By applying this approach across different datasets, we can determine which model is best suited for each case. This method is effective for exploring various classifiers and tuning them to achieve optimal performance.\n",
    "\n",
    "### <span style=\"color:red\">Task 5: Fill out the missing code - 7 points</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.datasets import (\n",
    "    load_iris,\n",
    "    load_wine,\n",
    "    load_digits,\n",
    "    make_moons,\n",
    "    make_classification,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load datasets\n",
    "datasets = {\n",
    "    \"Iris\": load_iris(return_X_y=True),\n",
    "    \"Wine\": load_wine(return_X_y=True),\n",
    "    \"Digits\": load_digits(return_X_y=True),\n",
    "    \"Moon\": make_moons(n_samples=1000, noise=0.3, random_state=42),  # Moon dataset\n",
    "    \"Hard Synthetic\": make_classification(  # Hard synthetic dataset\n",
    "        n_samples=1000,\n",
    "        n_features=50,\n",
    "        n_informative=25,\n",
    "        n_redundant=15,\n",
    "        n_classes=5,\n",
    "        n_clusters_per_class=2,\n",
    "        random_state=42,\n",
    "        flip_y=0.3,\n",
    "        class_sep=2,\n",
    "        weights=[0.2, 0.2, 0.2, 0.2, 0.2],\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Step 2: Define classifiers and hyperparameter grids\n",
    "classifiers = {\n",
    "    \"KNN\": #TODO: Define the KNN classifier\n",
    "    \"Logistic Regression\": # TODO: Define the Logistic Regression classifier\n",
    "    \"Decision Tree\": # TODO: Define the Decision Tree classifier\n",
    "    \"Random Forest\": RandomForestClassifier(n_jobs=-1),  \n",
    "    \"SVM\":  # TODO: Define Support Vector Machine classifier\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    \"KNN\": {\"n_neighbors\": [5, 7]},\n",
    "    \"Logistic Regression\": {\"C\": [0.1, 1]},\n",
    "    \"Decision Tree\": {\"max_depth\": [3, 5]},\n",
    "    \"Random Forest\": {\"n_estimators\": [50], \"max_depth\": [3]},\n",
    "    \"SVM\": {\"C\": [0.1, 1], \"kernel\": [\"linear\"]},\n",
    "}\n",
    "\n",
    "# Step 3: Initialize results storage\n",
    "overall_results = {}\n",
    "\n",
    "# Step 4: Process each dataset\n",
    "for dataset_name, (X, y) in datasets.items():\n",
    "    print(f\"\\nProcessing dataset: {dataset_name}\")\n",
    "    # Step 4.1: Split the dataset\n",
    "    X_train, X_test, y_train, y_test = # TODO: split the data in train/test. Set test_size to 0.3\n",
    "\n",
    "    # Step 4.2: Fit models and perform GridSearchCV\n",
    "    results = {}\n",
    "    model_performance = {\n",
    "        model_name: [] for model_name in classifiers.keys()\n",
    "    }  # Track performance\n",
    "    model_times = {\n",
    "        model_name: [] for model_name in classifiers.keys()\n",
    "    }  # Track training times\n",
    "\n",
    "    for model_name, model in classifiers.items():\n",
    "        # Record the start time of GridSearchCV\n",
    "        start_time = time.time()\n",
    "\n",
    "        \n",
    "        grid_search = # TODO: Use GridSearchCV for hyperparameter tuning. Use 3-fold cross validation\n",
    "        ____  # TODO: Fit the model using training data\n",
    "\n",
    "\n",
    "        # Record the end time of GridSearchCV\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time  # Time taken for the entire grid search\n",
    "\n",
    "        best_model = # TODO: Get the best model from GridSearchCV\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        # Store the results\n",
    "        results[model_name] = {\n",
    "            \"Best Params\": # TODO: Get the parameters of the best model from GridSearchCV\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Training Time (s)\": training_time,\n",
    "        }\n",
    "\n",
    "        # Append performance and training time\n",
    "        model_performance[model_name].append(accuracy)\n",
    "        model_times[model_name].append(training_time)\n",
    "\n",
    "    # Step 4.3: Print results for the current dataset\n",
    "    print(f\"Results for {dataset_name} Dataset:\")\n",
    "    for model_name, result in results.items():\n",
    "        print(\n",
    "            f\"  {model_name} - Best Params: {result['Best Params']} | Accuracy: {result['Accuracy']:.4f} | \"\n",
    "            f\"Training Time: {result['Training Time (s)']:.4f} seconds\"\n",
    "        )\n",
    "    print(\"\\n\" + \"-\" * 50 + \"\\n\")\n",
    "\n",
    "    # Step 4.4: Store overall results\n",
    "    overall_results[dataset_name] = results\n",
    "\n",
    "# Step 5: Print average performance across datasets\n",
    "print(\"Average Performance and Training Time per Model Across All Datasets:\")\n",
    "model_avg_performance = {}\n",
    "model_avg_times = {}\n",
    "\n",
    "for model_name in classifiers.keys():\n",
    "    all_accuracies = []\n",
    "    all_times = []\n",
    "    for dataset_name, dataset_results in overall_results.items():\n",
    "        all_accuracies.append(dataset_results[model_name][\"Accuracy\"])\n",
    "        all_times.append(dataset_results[model_name][\"Training Time (s)\"])\n",
    "    model_avg_performance[model_name] = # TODO: Compute average accuracy\n",
    "    model_avg_times[model_name] = # TODO: Compute average training time\n",
    "\n",
    "    print(\n",
    "        f\"{model_name}: Average Accuracy = {model_avg_performance[model_name]:.4f} | \"\n",
    "        f\"Average Training Time = {model_avg_times[model_name]:.4f} seconds\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which Model Seems to Be the Best for Each Dataset?  <span style=\"color:red\">Write down the list of best models - 1 point</span>\n",
    "\n",
    "\n",
    "After evaluating multiple models (KNN, Logistic Regression, Decision Trees, Random Forests, and SVMs) across several datasets, we can summarize the best-performing models for each dataset based on accuracy and training time. Write down which models achieved the highest accuracy. If there are multiple models, write them all down.\n",
    "\n",
    "#### **Iris Dataset**\n",
    "- **Best Model**: \n",
    "\n",
    "#### **Wine Dataset**\n",
    "- **Best Model**: \n",
    "\n",
    "#### **Digits Dataset**\n",
    "- **Best Model**: \n",
    "\n",
    "#### **Moon Dataset**\n",
    "- **Best Model**: \n",
    "\n",
    "#### **Synthetic Dataset**\n",
    "- **Best Model**: \n",
    "\n",
    "\n",
    "<span style=\"color:#4CAF50\">Congratulations on reaching the end of this assignment! Well done for completing all the assignments. Best of luck with your exams – keep up the great work and stay confident!</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
