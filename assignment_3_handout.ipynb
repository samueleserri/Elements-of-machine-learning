{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1176a0d7-bff8-4f08-8cf5-c2658991a516",
   "metadata": {},
   "source": [
    "# Assignment 3 - Generalization,  Model Selection and Beyond Linearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e50359-fcac-4af4-b83b-a71c7c21bc97",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "This assignment is worth a total of **10 points**. The goal of this assignment is to introduce you to selecting the best model using K fold cross validation. You will also explore methods for selecting hyperparameters to enhance the generalizability of your trained models.\n",
    "\n",
    "We have structured the assignment into three parts:\n",
    "\n",
    "1. **Part One**: Generalization\n",
    "2. **Part Two**: Model Selection\n",
    "3. **Part Three**: Beyond Linearity\n",
    "\n",
    "To ensure you understand how each package is used, libraries will be imported as and when needed. The libraries used are all open source, and if you do not have any of these libraries installed, you can install them using the `pip install` method, either via your terminal or within a code cell in this notebook. For example, in your code cell you can use:\n",
    "\n",
    "`!pip install matplotlib`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2cb8f9",
   "metadata": {},
   "source": [
    "# Part One: Generalization, <span style=\"color:green\">total of 3 points </span> \n",
    "\n",
    "\n",
    "We will make use of the \"make classification\" from sklearn datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c91924e",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cd31e8-b532-4b4e-85a5-da1c139ba9c1",
   "metadata": {},
   "source": [
    "### Make Classification Datasets and Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0df95a0-c84d-465e-9349-f116598eae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification, make_circles\n",
    "X, y = make_circles(n_samples=1000,factor=.25, noise=.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180b56e3-1e9d-4338-b94c-0a4a88f63afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset(X, y, title=\"Dataset\"):\n",
    "    \"\"\"\n",
    "    Plots the dataset with different colors for each class and adds a legend.\n",
    "    \n",
    "    Parameters:\n",
    "    X (array-like): Feature matrix.\n",
    "    y (array-like): Target vector.\n",
    "    title (str): Title of the plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.Paired)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"X 1\")\n",
    "    plt.ylabel(\"X 2\")\n",
    "    \n",
    "    # Create a legend\n",
    "    handles, labels = scatter.legend_elements()\n",
    "    legend_labels = ['Class 0: Negative', 'Class 1: Positive']\n",
    "    plt.legend(handles, legend_labels, title=\"Classes\")\n",
    "    \n",
    "    plt.show()\n",
    "plot_dataset(X, y, title=\"Generated Classification Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbbaf98",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Kfold Cross validation:\n",
    "\n",
    "\n",
    "\n",
    "### <span style=\"color:red\"> Task 1: Implement a K-Fold cross-validation function as discussed in **Lecture 6** by filling in the parts of the code with TODOs. </span>\n",
    "\n",
    "The kfold cross-validation metric is calculated as:\n",
    "\n",
    "$$\n",
    "CV_k = \\frac{1}{k} \\sum_{i=1}^{k} \\text{Metric}_i\n",
    "$$\n",
    "\n",
    "The metrics we would use will be accuracy and the f1 score. We will use the inbuilt libraries for these metrics directly from sklearn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf65b98-8401-4ddb-8a80-69fcce396148",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">Total: 1 point</span> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac92030-d436-4b65-a00c-a84f73daf99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def kfold_cross_validation(model, X, y, k=5, random_seed=0):\n",
    "    \"\"\"\n",
    "    K-Fold cross-validation for a selected model.\n",
    "\n",
    "    Parameters:\n",
    "    model: The model you want to evaluate.\n",
    "    X: The input features (numpy array or pandas DataFrame).\n",
    "    y: The target variable (numpy array or pandas Series).\n",
    "    k: Number of folds (default is 5).\n",
    "\n",
    "    Returns: \n",
    "    Average metrics across the K folds.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    n_samples = len(X)\n",
    "    # Get the fold size by dividing the number of samples by k\n",
    "    fold_size =  # TODO: Ensure integer division\n",
    "    \n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)  # Shuffle indices for random sampling\n",
    "\n",
    "    # Initialize lists to store accuracies and f1_scores\n",
    "    accuracies = # TODO: Accuracy list\n",
    "    f1_scores = # TODO: F1 score list\n",
    "\n",
    "    for i in range(k):\n",
    "        # Select the test and train indices\n",
    "        test_indices =  # TODO: Define test indices\n",
    "        train_indices =  # TODO: Define train indices\n",
    "        \n",
    "        X_train, X_test = X[train_indices], X[test_indices]\n",
    "        y_train, y_test = y[train_indices], y[test_indices]\n",
    "        \n",
    "        # TODO: Model fitting on train\n",
    "        # TODO: Use model to make Predictions on held out xtest\n",
    "        \n",
    "       \n",
    "        accuracy =  # TODO: Calculate accuracy and append to accuracies list\n",
    "        f1 =   # TODO: Calculate weighted F1 score and append to f1_scores list\n",
    "          \n",
    "    return # TODO: Return average accuracy and F1 score\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f3eeaa-7075-4790-b9e6-ab3f09daacab",
   "metadata": {},
   "source": [
    "### Model Fitting\n",
    "\n",
    "\n",
    "### <span style=\"color:red\">Task 2:  Fit 3 different models on your training data (i.e Logistic Regression, LDA, QDA) with your written 5-fold cross validation function above by filling in the parts of the code with TODO.</span>\n",
    "\n",
    "#### <span style=\"color:green\">Total: 1 point </span> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa40b7a-29a3-4c05-8842-25c98425c29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "\n",
    "# Instantiate your models\n",
    "logistic_regression = #TODO\n",
    "lda = #TODO\n",
    "qda = #TODO\n",
    "\n",
    "# Fit the models using k-fold cross-validation\n",
    "lr_accuracy, lr_f1 = #TODO\n",
    "lda_accuracy, lda_f1 = #TODO\n",
    "qda_accuracy, qda_f1 = #TODO\n",
    "\n",
    "\n",
    "print(f'Logistic Regression: Accuracy = {lr_accuracy:.2f}, F1 Score = {lr_f1:.2f}')\n",
    "print(f'LDA: Accuracy = {lda_accuracy:.2f}, F1 Score = {lda_f1:.2f}')\n",
    "print(f'QDA: Accuracy = {qda_accuracy:.2f}, F1 Score = {qda_f1:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff5da29",
   "metadata": {},
   "source": [
    "### Evaluating Model Performance with Varying K\n",
    "\n",
    "### <span style=\"color:red\">Task 2.1: Write a for loop over different values of k by completing the sections of the code with TODO. Based on the output of your code, which model is the best and at which value of k?\n",
    "\n",
    "#### <span style=\"color:green\">Total: 0.5 points </span> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb0a1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of k values\n",
    "k_values = [5, 10, 15, 20, 25, 30]\n",
    "\n",
    "# TODO:Initialize lists to store accuracy and f1 score of each model\n",
    "\n",
    "# Perform cross-validation for each value of k\n",
    "for k in k_values:\n",
    "    lr_accuracy, lr_f1 = #TODO: Use written kfold_cross_validation  for the logistic regression model\n",
    "    lda_accuracy, lda_f1 = #TODO:  Use written kfold_cross_validation  for the lda model\n",
    "    qda_accuracy, qda_f1 = #TODO: Use written kfold_cross_validation  for the qda model\n",
    "    \n",
    "    lr_accuracies.append(lr_accuracy)\n",
    "    lr_f1_scores.append(lr_f1)\n",
    "    lda_accuracies.append(lda_accuracy)\n",
    "    lda_f1_scores.append(lda_f1)\n",
    "    qda_accuracies.append(qda_accuracy)\n",
    "    qda_f1_scores.append(qda_f1)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# accuracy plot\n",
    "plt.subplot(1, 2, 1)\n",
    "# TODO: Plot the accuracy of each model\n",
    "\n",
    "# F1 score plot\n",
    "plt.subplot(1, 2, 2)\n",
    "# TODO: Plot the F1 score of each model\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65fd2ae-ae14-4f6d-87c2-70a17128db30",
   "metadata": {},
   "source": [
    "### Decision Boundary Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b4685b-0ddd-4ba3-9c8f-00e0f107356b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "###  <span style=\"color:red\">Task 3: Looking at the generated plots of the decision boundaries for the different models, can you write down what you observe </span>\n",
    "\n",
    "\n",
    "#### <span style=\"color:green\">Total: 0.5 point for explanation </span> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54296bf3-268b-4f0d-87f5-2856b0fad5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y, ax, title):\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, alpha=0.3)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o')\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b92e9d-f244-45f4-a0b2-44c2684f9929",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "plot_decision_boundary(logistic_regression, X, y, axes[0], \"Logistic Regression Decision Boundary\")\n",
    "plot_decision_boundary(lda, X, y, axes[1], \"LDA Decision Boundary\")\n",
    "plot_decision_boundary(qda, X, y, axes[2], \"QDA Decision Boundary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1977f69d-c3e3-46b1-8af9-66ce18a720ba",
   "metadata": {},
   "source": [
    "# Part Two: Model Selection, <span style=\"color:green\">total of 3 points </span> \n",
    "\n",
    "In this section, we will:\n",
    "\n",
    "* Fit Lasso and Ridge regression models.\n",
    "* How to select the optimal $\\lambda$ for each model.\n",
    "* Compare the performance of the two models using the optimal $\\lambda$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5975aa-064a-4d6e-9159-8d3cfdf22fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_regression_dataset(X, y, title=\"Regression Dataset\"):\n",
    "    \"\"\"\n",
    "    Plots the regression dataset with a color gradient representing the target values.\n",
    "    Parameters:\n",
    "    X (array-like): Feature matrix.\n",
    "    y (array-like): Target vector.\n",
    "    title (str): Title of the plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolors='k')\n",
    "    plt.colorbar(label='Target Value')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"X 1\")\n",
    "    plt.ylabel(\"X 2\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd0fee2-a21f-4138-9799-6c731aca8324",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47135c3-e5e2-4e35-b1d2-ed60cc5a7bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X_regression, y_regression = make_regression(n_samples=1000, n_features=50, n_informative=15, noise=0.35, random_state=42)\n",
    "plot_regression_dataset(X_regression, y_regression, title=\"Generated Regression Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9191e1-492c-4fde-87d0-4740228581ba",
   "metadata": {},
   "source": [
    "### Lasso & Ridge Regression\n",
    "\n",
    "\n",
    "\n",
    "### <span style=\"color:red\">Task 4: Fill in the missing parts of the code to fit ridge and lasso regression using different lambda values. </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4b5311-a149-4bdb-86f0-739509eda85e",
   "metadata": {},
   "source": [
    "\n",
    "#### <span style=\"color:green\">Total: 1 point </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae8ad61-f640-4781-a195-6c2454491235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_regression, y_regression, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define a range of lambda values\n",
    "lambda_values = np.logspace(-4, 4, 100)\n",
    "mse_ridge = []\n",
    "mse_lasso = []\n",
    "\n",
    "# Define K-Fold cross-validation - NOTE we using the inbuilt KFold here\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform K-Fold Cross-Validation for Ridge Regression\n",
    "for lambda_val in lambda_values:\n",
    "    # TODO: Fit a Ridge regression model with the current lambda value\n",
    "    mse_scores = # TODO: Use cross_val_score with 'neg_mean_squared_error'\n",
    "    mse_ridge.append(-mse_scores.mean())  # Negate to get positive MSE\n",
    "\n",
    "# Perform K-Fold Cross-Validation for Lasso Regression\n",
    "for lambda_val in lambda_values:\n",
    "    # TODO: Fit a Lasso regression model with the current lambda value\n",
    "    mse_scores = # TODO: Use cross_val_score with 'neg_mean_squared_error'\n",
    "    mse_lasso.append(-mse_scores.mean())  # Negate to get positive MSE\n",
    "\n",
    "# Plot MSE vs. Lambda for Ridge and Lasso Regression\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.figure(figsize=(10, 6))\n",
    "# TODO: Plot the MSE for Ridge Regression (lambda values vs. mse_ridge)\n",
    "# TODO: Plot the MSE for Lasso Regression (lambda values vs. mse_lasso)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Ridge vs. Lasso Regression: MSE vs. Lambda')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Choose the optimal lambda for each model\n",
    "optimal_lambda_ridge = # TODO: Select the best lambda value for Ridge\n",
    "optimal_lambda_lasso = # TODO: Select the best lambda value for Lasso\n",
    "print(f'Optimal Lambda for Ridge Regression: {optimal_lambda_ridge:.3f}')\n",
    "print(f'Optimal Lambda for Lasso Regression: {optimal_lambda_lasso:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30df32d2-e58b-4282-bbd5-4211142c83f6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "###  <span style=\"color:red\">Task 5: Complete the code parts with TODOs and answer based on the output of your model, which model will you choose and why? </span>\n",
    "\n",
    "Using the optimal lambda values obtained from cross-validation for both Ridge and Lasso regression, follow these steps:\n",
    "\n",
    "* Fit the Ridge and Lasso regression on your train data using the selected optimal lambda values.\n",
    "* Evaluate the performance of the models on the testing set.\n",
    "* Between the two models, which will you choose and why? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7490717-b311-40dc-b726-dc41c6fd0097",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">Total: 2 points </span> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541b9cd1-7b50-42b9-8ed1-bb44edcbe34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# TODO: Fit Ridge Regression on train data with optimal lambda\n",
    "y_pred_ridge = ridge.predict(X_test)\n",
    "mse_ridge = #TODO: estimate the MSE\n",
    "\n",
    "# TODO: Fit Lasso Regression train data with optimal lambda\n",
    "y_pred_lasso = lasso.predict(X_test)\n",
    "mse_lasso = #TODO: estimate the MSE\n",
    "\n",
    "print(f\"Lasso regression with lambda {optimal_lambda_lasso:.3f} has mse {mse_lasso:.3f} and ridge regression using lambda {optimal_lambda_ridge:.3f} has mse {mse_ridge:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09406e20",
   "metadata": {},
   "source": [
    "# Part Three: Beyond Linearity,  <span style=\"color:green\">total of 4 points </span> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b525a9-4669-474c-9b33-10b171a40be7",
   "metadata": {},
   "source": [
    "### Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d2dc71-cad5-453d-a2a1-04104bb2b07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "num_samples = 1000\n",
    "X_splines = np.sort(np.random.rand(num_samples) * 10)\n",
    "y_splines = np.sin(X_splines) + np.random.randn(num_samples) * 0.1\n",
    "\n",
    "# Plot the synthetic data\n",
    "plt.scatter(X_splines, y_splines, label='Data')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Synthetic Data')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0a07fa-2bef-4862-b659-e464e2114450",
   "metadata": {},
   "source": [
    "### Exploring Degrees Polynomial Regression\n",
    "\n",
    "\n",
    " ###  <span style=\"color:red\">Task 6: Fit polynomial regresion using different degrees. Complete all parts indicating TODOs. Based on your output, which degree would you select as best and why? </span>\n",
    "\n",
    "\n",
    "#### <span style=\"color:green\">Total: 2 points </span> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a431198d-b88e-440e-96d4-9d675369a51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "plt.figure(figsize=(12, 8)) \n",
    "degrees_range = [2, 4, 5, 6]\n",
    "\n",
    "# Loop through different polynomial degrees\n",
    "for j, degree in enumerate(degrees_range):\n",
    "    poly_features = # TODO: Create polynomial transformer\n",
    "    X_poly = # TODO: Transform the features to polynomial features\n",
    "    \n",
    "    # TODO: Fit the polynomial regression model\n",
    "    \n",
    "    # Predict the output\n",
    "    y_poly_pred = poly_model.predict(X_poly)\n",
    "\n",
    "    # Calculate the MSE for polynomial regression\n",
    "    mse_poly = mean_squared_error(y_splines, y_poly_pred)\n",
    "\n",
    "    # Plot the polynomial regression\n",
    "    plt.subplot(2, 2, j + 1)  # 2 rows, 2 columns\n",
    "    plt.scatter(X_splines, y_splines, label='Data', color='blue', alpha=0.5)\n",
    "    plt.plot(X_splines, y_poly_pred, label=f'Polynomial Regression (degree={degree})', color='red')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('y')\n",
    "    plt.title(f'Polynomial Regression (Degree={degree}) - MSE: {mse_poly:.3f}')\n",
    "    plt.legend()\n",
    "    plt.grid(True) \n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9594b49a-8ff2-4633-a429-c5af95742487",
   "metadata": {},
   "source": [
    "### Exploring Degrees and Knots on Spline Regression\n",
    "\n",
    "###  <span style=\"color:red\">Task 7: Experiment with various degrees and knots. Here you will need to first complete the missing parts of the code. then answer the following questions based on your code output:</span>\n",
    "\n",
    "* What observations can you make regarding the effect of varying knots?\n",
    "* Which specific combination of knots and degrees would you select?\n",
    "\n",
    "#### <span style=\"color:green\">Total: 2 points </span> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487ca84f-be37-43a5-ba63-aee911d7030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import SplineTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "knots_range = [2, 3, 4, 10]\n",
    "degrees_range = [2, 3, 10]\n",
    "results = []\n",
    "# Loop through different numbers of knots and degrees\n",
    "for i, n_knots in enumerate(knots_range):\n",
    "    for j, degree in enumerate(degrees_range):\n",
    "        spline_transformer =   # TODO: Create a spline transformer\n",
    "        spline_model = # TODO: Create linear regression pipeline\n",
    "\n",
    "        # TODO: Fit the spline regression model\n",
    "        \n",
    "        y_spline_pred = spline_model.predict(X_splines.reshape(-1, 1))\n",
    "        mse_spline = mean_squared_error(y_splines, y_spline_pred)\n",
    "        \n",
    "        # TODO:  Append knots, degree, and mse to results\n",
    "\n",
    "        plt.subplot(len(knots_range), len(degrees_range), i * len(degrees_range) + (j + 1))\n",
    "        plt.scatter(X_splines, y_splines, label='Data', color='blue')\n",
    "        plt.plot(X_splines, y_spline_pred, label=f'Spline (knots={n_knots}, degree={degree})', color='red')\n",
    "        \n",
    "        plt.xlabel('X')\n",
    "        plt.ylabel('y')\n",
    "        plt.title(f'Knots={n_knots}, Degree={degree} - MSE {mse_spline:.3f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eefb3ab-6caf-4861-951d-3f143d48b20e",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\">\n",
    "\n",
    "\n",
    "# That is it for this assignment, we do hope you learn something from this exercise!\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
